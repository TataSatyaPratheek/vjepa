# @package _global_
defaults:
  - _self_
  - model: vjepa_tiny
  - data: hmdb51
  - trainer: m1_optimized
  - callbacks: default
  - logger: null
  - paths: default
  - hydra: default

seed: 42
task_name: "vjepa_hmdb51"
tags: ["v_jepa", "m1_8gb_optimized", "predictive_architecture"]

train:
  enable: True
  resume: False
  ckpt_path: null

model:
  encoder:
    name: "vit_tiny"
    pretrained: "google/vit-base-patch16-224"
    frozen: True
    gradient_checkpointing: True
  
  target_encoder:
    frozen: True
    momentum_teacher: 0.996
  
  predictor:
    hidden_dim: 192
    num_layers: 4
    dropout: 0.1

  latent_dim: 192
  mask_ratio: 0.75
  tube_mask: True
  
  # V-JEPA specific parameters
  momentum:
    start: 0.996
    end: 1.0

data:
  root_dir: "data/hmdb51/processed"
  clip_length: 8
  frame_size: 224  # Reduced from 128 for M1 memory
  num_workers: 2
  persistent_workers: False
  batch_size: 1  # Reduced for M1 8GB
  num_frames: 8  # Reduced from 16 for M1 memory
  frame_rate: 5

optimization:
  lr: 5e-5  # Lower learning rate for stability
  weight_decay: 0.01
  max_epochs: 30  # Fewer epochs for M1
  warmup_epochs: 10
  grad_clip: 0.5
  use_amp: True
  accumulate_grad_batches: 4  # Gradient accumulation for effective batch size
  
  # V-JEPA momentum scheduling
  momentum_scheduler:
    type: "cosine"
    start_value: 0.996
    end_value: 1.0

logging:
  feature_viz_interval: 2000
  save_predictions: 4
  log_momentum: True
